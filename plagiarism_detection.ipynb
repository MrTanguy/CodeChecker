{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No.JS\\n// DDDDDDDDDDDDdd\\n/* HEHEBOY \\n\\n\\n\\nSDqsd\\n*/', 'Content3\\n\\n// Normal Commentary', 'yes\\n// LE CHOCOLAT\\n// SQDJQSDIQSDISDISDISJDISQOQIDQSOIDQSIDJQSIDJSIOJDISQDJIOSQDOIQSDJIQSDOQSJD0IQSDOKSDOIQSJODQSJDOQKDJs', 'HEHE']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from numpy import vectorize \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import plotly.graph_objects as go\n",
    "import string\n",
    "\n",
    "s=\"\"\n",
    "sample_files = []\n",
    "load_dotenv()\n",
    "\n",
    "Space_Cleared_Exts = os.environ[\"EXTENSIONS\"].replace(\" \", \"\")\n",
    "# ------------------------------------------------------------------\n",
    "# Parcourt tout les fichiers finissant avec les extensions dans .env\n",
    "# ------------------------------------------------------------------\n",
    "for ext in os.environ[\"EXTENSIONS\"] :\n",
    "    if(ext==\",\") :\n",
    "        ext=\"[\"\n",
    "        for doc in os.listdir(\"./FilesToTest/\") :\n",
    "            if doc.endswith(str(s)) :\n",
    "                sample_files.append(''.join(doc))\n",
    "        s = \"\"\n",
    "    if(ext!=\"[\" and ext!=\"]\") :\n",
    "        s+=ext\n",
    "for doc in os.listdir(\"./FilesToTest/\") :\n",
    "    if doc.endswith(str(s.replace(\" \",\"\"))) :\n",
    "        sample_files.append(''.join(doc))\n",
    " \n",
    "sample_contents = [open(\"./FilesToTest/\"+File).read() for File in sample_files]\n",
    "print(sample_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample_ContentsBis = []\n",
    "Commentary_Content = []\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# Parcourt notre texte et enleve les commentaires si le texte vient d'un fichier JS\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "def PurgeJSCommentary(word) :\n",
    "        ReelText = \"\"\n",
    "        CommText = \"\"\n",
    "        FoundComm = False\n",
    "        FoundHugeComm = False\n",
    "        for char in range(len(sample_contents[word])) :\n",
    "            if(FoundComm == False) :\n",
    "                if(char+1 < len(sample_contents[word])) :\n",
    "                    if(sample_contents[word][char] == \"/\" and sample_contents[word][char+1] == \"/\") :\n",
    "                        FoundComm=True\n",
    "                    elif(sample_contents[word][char]==\"/\" and sample_contents[word][char+1]==\"*\") :\n",
    "                        FoundHugeComm=True\n",
    "            \n",
    "            if(FoundComm==False and FoundHugeComm == False) :\n",
    "                ReelText+=sample_contents[word][char]\n",
    "            else :\n",
    "                CommText+=sample_contents[word][char]\n",
    "            if(FoundComm and FoundHugeComm == False) :\n",
    "                if(sample_contents[word][char] == \"\\n\") :\n",
    "                    FoundComm = False\n",
    "            if(FoundHugeComm) :\n",
    "                if(sample_contents[word][char]==\"*\" and sample_contents[word][char]==\"\\\\\") :\n",
    "                    FoundHugeComm = False\n",
    "        Commentary_Content.append(CommText)\n",
    "        Sample_ContentsBis.append(ReelText)\n",
    "        \n",
    "for i in range(len(sample_files)) :\n",
    "    if(sample_files[i].endswith(\".js\")) :\n",
    "        PurgeJSCommentary(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No.JS\\n', 'Content3\\n\\n', 'yes\\n']\n",
      "['// DDDDDDDDDDDDdd\\n/* HEHEBOY \\n\\n\\n\\nSDqsd\\n*/', '// Normal Commentary', '// LE CHOCOLAT\\n// SQDJQSDIQSDISDISDISJDISQOQIDQSOIDQSIDJQSIDJSIOJDISQDJIOSQDOIQSDJIQSDOQSJD0IQSDOKSDOIQSJODQSJDOQKDJs']\n"
     ]
    }
   ],
   "source": [
    "print(Sample_ContentsBis)\n",
    "print(Commentary_Content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize() :\n",
    "    return \"\"\n",
    "tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no': 2, 'js': 1, 'content3': 0, 'yes': 3}\n"
     ]
    }
   ],
   "source": [
    "# vectorize = lambda Text: TfidfVectorizer().fit_transform(Text).toarray()\n",
    "\n",
    "x = TfidfVectorizer()\n",
    "x.fit_transform(Sample_ContentsBis)\n",
    "print(x.vocabulary_)\n",
    "def vectorize(Text) :\n",
    "    return TfidfVectorizer().fit_transform(Text).toarray()\n",
    "\n",
    "# similarity = lambda doc1, doc2: cosine_similarity([doc1, doc2])\n",
    "\n",
    "def similarity(doc1,doc2) :\n",
    "    return cosine_similarity([doc1,doc2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectors = vectorize(Sample_ContentsBis)\n",
    "s_vectors = list(zip(sample_files, vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_plagiarism():\n",
    "    results = set()\n",
    "    global s_vectors\n",
    "    for sample_a, text_vector_a in s_vectors:\n",
    "        new_vectors = s_vectors.copy()\n",
    "        current_index = new_vectors.index((sample_a, text_vector_a))\n",
    "        del new_vectors[current_index]\n",
    "        for sample_b, text_vector_b in new_vectors:\n",
    "            sim_score = similarity(text_vector_a, text_vector_b)[0][1]\n",
    "            sample_pair = sorted((sample_a, sample_b))\n",
    "            score = sample_pair[0], sample_pair[1], sim_score\n",
    "            results.add(score)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('no.js', 'testingfile.js', 0.0)\n",
      "('testingfile.js', 'yes.js', 0.0)\n",
      "('no.js', 'yes.js', 0.0)\n"
     ]
    }
   ],
   "source": [
    "for data in check_plagiarism():\n",
    "    print(data)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
